{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c30b865",
   "metadata": {},
   "source": [
    "# Stage 1: Downloading Election Data from the Web\n",
    "\n",
    "## The Goal\n",
    "\n",
    "We need precinct-level election results from Iowa's Secretary of State website for 2016, 2018, and 2020. Each county has its own Excel file, and there are 99 counties √ó 3 years = **297 files** to download.\n",
    "\n",
    "Doing this by hand would take hours of clicking. Instead, we'll write code to automate it.\n",
    "\n",
    "## The Big Picture (Language-Agnostic)\n",
    "\n",
    "Whether you're used to **Stata**, **R**, or **Python**, the logic is the same:\n",
    "\n",
    "1. **Loop** through each year (2016, 2018, 2020)\n",
    "2. **Go to** the webpage listing that year's files\n",
    "3. **Find** all the download links for Excel files\n",
    "4. **Download** each file\n",
    "5. **Rename** it to include the year (so we know which election it's from)\n",
    "\n",
    "The code below does exactly this. We'll try a simple approach first (which fails), then use a more powerful tool that works.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde60c1",
   "metadata": {},
   "source": [
    "## Key Concepts (For Stata/R Users)\n",
    "\n",
    "### Loops: Repeating Actions\n",
    "\n",
    "| Stata | R | Python |\n",
    "|-------|---|--------|\n",
    "| `foreach year in 2016 2018 2020 {` | `for (year in c(2016, 2018, 2020)) {` | `for year in [2016, 2018, 2020]:` |\n",
    "| `    ...` | `    ...` | `    ...` |\n",
    "| `}` | `}` | |\n",
    "\n",
    "### Building Text (String Concatenation)\n",
    "\n",
    "Creating a URL by combining pieces:\n",
    "\n",
    "| Stata | R | Python |\n",
    "|-------|---|--------|\n",
    "| `\"https://site.com/\" + \"\\`year'\"` | `paste0(\"https://site.com/\", year)` | `f\"https://site.com/{year}\"` |\n",
    "\n",
    "### Conditional Logic\n",
    "\n",
    "| Stata | R | Python |\n",
    "|-------|---|--------|\n",
    "| `if x == 1 {` | `if (x == 1) {` | `if x == 1:` |\n",
    "\n",
    "### What is HTML?\n",
    "\n",
    "Webpages are written in **HTML** ‚Äî a structured text format with \"tags\":\n",
    "\n",
    "```html\n",
    "<a href=\"https://example.com/Adair_2018.xls\">Download Adair County</a>\n",
    "```\n",
    "\n",
    "- `<a>` = a link (anchor)\n",
    "- `href=\"...\"` = where the link points to\n",
    "- The text between tags is what you see on the page\n",
    "\n",
    "**BeautifulSoup** (Python) and **rvest** (R) are tools that read HTML and let you search for specific tags ‚Äî like finding all the `<a>` links on a page.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505c93e",
   "metadata": {},
   "source": [
    "## Setup: Install Required Tools\n",
    "\n",
    "First, we install the packages (libraries) we'll need. \n",
    "\n",
    "- **requests** ‚Äî downloads web pages (like `httr` in R)\n",
    "- **beautifulsoup4** ‚Äî parses HTML to find links (like `rvest` in R)\n",
    "- **openpyxl** / **xlrd** ‚Äî reads Excel files (like `readxl` in R or `import excel` in Stata)\n",
    "\n",
    "Run this cell once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2888d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages (only need to run once)\n",
    "# The \"!\" means \"run this as a terminal command\"\n",
    "!pip install requests beautifulsoup4 openpyxl xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831dfac4",
   "metadata": {},
   "source": [
    "## First Attempt: Simple Web Scraping\n",
    "\n",
    "The code below tries to:\n",
    "\n",
    "1. **Build the URL** for each year's results page\n",
    "2. **Download the HTML** of that page\n",
    "3. **Parse the HTML** to find all links\n",
    "4. **Filter** to keep only Excel files for that year\n",
    "5. **Download each file** and save it\n",
    "\n",
    "### Reading the Code\n",
    "\n",
    "```python\n",
    "for year in years:                    # Loop through each year\n",
    "    url = f\"...{year}...\"             # Build the URL (like paste0 in R)\n",
    "    response = requests.get(url)       # Download the page\n",
    "    soup = BeautifulSoup(response.text) # Parse the HTML\n",
    "    \n",
    "    for link in soup.find_all('a'):   # Find all <a> tags (links)\n",
    "        href = link['href']            # Get the URL from the link\n",
    "        if href.endswith('.xls'):      # Check if it's an Excel file\n",
    "            # Download it...\n",
    "```\n",
    "\n",
    "### What to Expect\n",
    "\n",
    "**This will fail!** You'll see \"403 Forbidden\" errors. That's intentional ‚Äî we'll learn why and fix it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dcbdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "First attempt: Download Iowa election results using requests + BeautifulSoup\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - What years do we want?\n",
    "# =============================================================================\n",
    "\n",
    "years = [\"2016\", \"2018\", \"2020\"]\n",
    "output_folder = Path(\"./iowa_election_results\")\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "print(f\"Files will be saved to: {output_folder.absolute()}\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN LOOP - Go through each year\n",
    "# =============================================================================\n",
    "\n",
    "for year in years:\n",
    "    \n",
    "    # Build the URL for this year's page\n",
    "    # This is like: paste0(\"https://sos.iowa.gov/precinct-results-county-\", year, \"-general\") in R\n",
    "    url = f\"https://sos.iowa.gov/precinct-results-county-{year}-general\"\n",
    "    print(f\"Processing {year}...\")\n",
    "    print(f\"  URL: {url}\")\n",
    "    \n",
    "    # Download the webpage\n",
    "    # This is like: httr::GET(url) in R\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the page loaded successfully\n",
    "    # Status 200 = OK, 403 = Forbidden, 404 = Not Found\n",
    "    if response.status_code != 200:\n",
    "        print(f\"  ERROR: Could not load page (status {response.status_code})\")\n",
    "        continue  # Skip to next year (like \"next\" in R loops)\n",
    "    \n",
    "    # Parse the HTML to find all links\n",
    "    # BeautifulSoup reads the HTML structure so we can search it\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all links (<a> tags) that point to Excel files\n",
    "    excel_links = []\n",
    "    for link in soup.find_all('a', href=True):  # Find all <a> tags that have an href\n",
    "        href = link['href']                       # Get the URL\n",
    "        \n",
    "        # Check if this link is to an Excel file for our year\n",
    "        is_excel = href.lower().endswith('.xls') or href.lower().endswith('.xlsx')\n",
    "        is_correct_year = f\"{year}general\" in href.lower()\n",
    "        \n",
    "        if is_excel and is_correct_year:\n",
    "            excel_links.append(href)\n",
    "    \n",
    "    print(f\"  Found {len(excel_links)} Excel files\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULT: Downloads failed due to 403 errors.\")\n",
    "print(\"The website detected we're a script, not a human!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417b2b6",
   "metadata": {},
   "source": [
    "## Why Did It Fail?\n",
    "\n",
    "We got **403 Forbidden** errors. What happened?\n",
    "\n",
    "### HTTP Status Codes\n",
    "\n",
    "When you request a webpage, the server sends back a status code:\n",
    "\n",
    "| Code | Meaning |\n",
    "|------|---------|\n",
    "| 200 | Success ‚Äî here's the page |\n",
    "| 403 | Forbidden ‚Äî I'm refusing to serve you |\n",
    "| 404 | Not Found ‚Äî page doesn't exist |\n",
    "\n",
    "### Bot Protection\n",
    "\n",
    "Many websites check **who** is asking for the page:\n",
    "\n",
    "- Real browser? ‚úÖ Allowed\n",
    "- Python script? ‚ùå Blocked\n",
    "\n",
    "They do this by looking at **headers** ‚Äî extra information sent with each request. Your browser sends headers like:\n",
    "\n",
    "```\n",
    "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0...\n",
    "```\n",
    "\n",
    "But Python's `requests` library sends:\n",
    "\n",
    "```\n",
    "User-Agent: python-requests/2.31.0\n",
    "```\n",
    "\n",
    "The website sees \"python-requests\" and says \"Nope, you're a bot!\"\n",
    "\n",
    "### Solutions\n",
    "\n",
    "1. **Add fake headers** ‚Äî pretend to be a browser (sometimes works)\n",
    "2. **Use a real browser** ‚Äî control Chrome/Edge with Selenium (always works)\n",
    "\n",
    "We'll skip option 1 (this website blocks it too) and go straight to Selenium.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ccbae",
   "metadata": {},
   "source": [
    "## The Solution: Browser Automation with Selenium\n",
    "\n",
    "**Selenium** is a tool that lets Python control a real web browser (Chrome, Edge, or Firefox). \n",
    "\n",
    "Instead of Python pretending to be a browser, Python actually *operates* a browser ‚Äî clicking buttons, navigating pages, and downloading files just like a human would.\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "The website can't tell the difference because it IS a real browser! All the JavaScript runs, cookies work normally, and the browser fingerprint looks legitimate.\n",
    "\n",
    "### The Logic is the Same\n",
    "\n",
    "Remember our original approach?\n",
    "\n",
    "1. Loop through years\n",
    "2. Go to the webpage\n",
    "3. Find Excel links\n",
    "4. Download each file\n",
    "\n",
    "**Selenium does the exact same thing** ‚Äî we just swap out the tools:\n",
    "\n",
    "| Task | Before (requests) | After (Selenium) |\n",
    "|------|-------------------|------------------|\n",
    "| Go to webpage | `requests.get(url)` | `driver.get(url)` |\n",
    "| Find links | `soup.find_all('a')` | `driver.find_elements(By.TAG_NAME, 'a')` |\n",
    "| Download file | `requests.get(file_url)` | Browser downloads automatically |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd63590",
   "metadata": {},
   "source": [
    "## Install Selenium\n",
    "\n",
    "Run this cell to install the browser automation tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b90fb",
   "metadata": {},
   "source": [
    "## Choose Your Browser\n",
    "\n",
    "Selenium can control Chrome, Edge, or Firefox. **Change the line below** to match what you have installed:\n",
    "\n",
    "- **Edge** is pre-installed on Windows\n",
    "- **Chrome** is most common\n",
    "- **Firefox** also works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200baff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BROWSER SELECTION - Change this to match what you have!\n",
    "# ============================================================\n",
    "\n",
    "BROWSER = \"chrome\"   # Options: \"chrome\", \"edge\", \"firefox\"\n",
    "\n",
    "print(f\"‚úÖ Will use: {BROWSER.title()}\")\n",
    "print(f\"   (If you don't have {BROWSER.title()}, change BROWSER above and re-run this cell)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8ff59",
   "metadata": {},
   "source": [
    "## The Working Code\n",
    "\n",
    "This cell does the full download. It will:\n",
    "\n",
    "1. Open a browser window (or run \"headless\" if there's no display)\n",
    "2. Navigate to each year's page\n",
    "3. Find all Excel file links\n",
    "4. Download each file\n",
    "5. Rename files with the year (e.g., `Adair.xls` ‚Üí `Adair_2018.xls`)\n",
    "\n",
    "**This takes 10-15 minutes** because we pause between downloads to be polite to the server.\n",
    "\n",
    "### Note: Headless Mode Detection\n",
    "\n",
    "The code automatically detects if you're running in a cloud environment (no monitor):\n",
    "\n",
    "```python\n",
    "headless = os.environ.get('CODESPACES') == 'true' or (os.environ.get('DISPLAY') is None and os.name != 'nt')\n",
    "```\n",
    "\n",
    "| Check | What it catches |\n",
    "|-------|-----------------|\n",
    "| `CODESPACES == 'true'` | GitHub Codespaces |\n",
    "| `DISPLAY is None` | Any Linux server without a monitor (Gitpod, AWS, Replit, etc.) |\n",
    "| `os.name != 'nt'` | Excludes Windows (which doesn't use DISPLAY) |\n",
    "\n",
    "**On your laptop:** You'll see the browser window open and navigate automatically.\n",
    "\n",
    "**In the cloud:** The browser runs invisibly (\"headless\") since there's no screen to display it.\n",
    "\n",
    "---\n",
    "\n",
    "### A Note on \"Practical Plumbing\"\n",
    "\n",
    "The code includes two helper functions ‚Äî `wait_for_download` and `clear_folder` ‚Äî that handle messy real-world details:\n",
    "\n",
    "**`wait_for_download`**: When you download manually, you wait for it to finish before clicking the next link. Computers don't wait automatically, so this function checks every half-second: *\"Is the download done yet?\"*\n",
    "\n",
    "**`clear_folder`**: Before each download, we empty the temp folder. Why? So when a new file appears, we know it's the one we just downloaded (not an old one). In plain English this is just \"delete all files in folder\" ‚Äî but Python has no simple command for this, so we loop through and delete each file individually:\n",
    "\n",
    "```python\n",
    "for f in folder.iterdir():   # Loop through each file\n",
    "    try:\n",
    "        f.unlink()           # Delete it (.unlink = delete)\n",
    "    except:\n",
    "        pass                 # If it fails, skip it\n",
    "```\n",
    "\n",
    "**The bigger point:** If you're thinking \"this seems like a lot of code for something simple\" ‚Äî you're right! The core logic (loop through years, find links, download files) is straightforward. The extra code handles all the timing, file management, and edge cases that reality throws at you. Introductory courses rarely show this stuff, but it's what real automation requires.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Download Iowa election results using Selenium (browser automation).\n",
    "Supports Chrome, Edge, and Firefox.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, unquote\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# =============================================================================\n",
    "# SETUP FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def setup_driver(download_dir: str):\n",
    "    \"\"\"\n",
    "    Set up the web browser for automated downloading.\n",
    "    Uses the BROWSER variable you set in the previous cell.\n",
    "    \"\"\"\n",
    "    browser = BROWSER.lower()\n",
    "    download_path = str(Path(download_dir).absolute())\n",
    "    \n",
    "    # Check if we're in a headless environment (no display)\n",
    "    headless = os.environ.get('CODESPACES') == 'true' or (os.environ.get('DISPLAY') is None and os.name != 'nt')\n",
    "    \n",
    "    if headless:\n",
    "        print(\"Running in headless mode (no visible browser window)\")\n",
    "    \n",
    "    # Set up the browser based on user's choice\n",
    "    if browser == \"chrome\":\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        options = Options()\n",
    "        options.add_experimental_option(\"prefs\", {\n",
    "            \"download.default_directory\": download_path,\n",
    "            \"download.prompt_for_download\": False,\n",
    "        })\n",
    "        if headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        \n",
    "    elif browser == \"edge\":\n",
    "        from selenium.webdriver.edge.options import Options\n",
    "        options = Options()\n",
    "        options.add_experimental_option(\"prefs\", {\n",
    "            \"download.default_directory\": download_path,\n",
    "            \"download.prompt_for_download\": False,\n",
    "        })\n",
    "        if headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        driver = webdriver.Edge(options=options)\n",
    "        \n",
    "    elif browser == \"firefox\":\n",
    "        from selenium.webdriver.firefox.options import Options\n",
    "        options = Options()\n",
    "        options.set_preference(\"browser.download.folderList\", 2)\n",
    "        options.set_preference(\"browser.download.dir\", download_path)\n",
    "        options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \n",
    "            \"application/vnd.ms-excel,application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\")\n",
    "        if headless:\n",
    "            options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Firefox(options=options)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown browser: {browser}. Use 'chrome', 'edge', or 'firefox'\")\n",
    "    \n",
    "    print(f\"‚úÖ Started {browser.title()} browser\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "def get_excel_links(driver, page_url: str) -> list:\n",
    "    \"\"\"\n",
    "    Navigate to a page and find all Excel file links.\n",
    "    \n",
    "    This is the Selenium version of what we did with BeautifulSoup:\n",
    "    - driver.get(url) instead of requests.get(url)\n",
    "    - driver.find_elements() instead of soup.find_all()\n",
    "    \"\"\"\n",
    "    print(f\"  Navigating to: {page_url}\")\n",
    "    driver.get(page_url)\n",
    "    time.sleep(3)  # Wait for page to load\n",
    "    \n",
    "    # Find all <a> tags (links) on the page\n",
    "    links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "    \n",
    "    # Filter to keep only Excel files\n",
    "    excel_links = []\n",
    "    for link in links:\n",
    "        try:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if href and (href.lower().endswith('.xls') or href.lower().endswith('.xlsx')):\n",
    "                filename = unquote(Path(urlparse(href).path).name)\n",
    "                excel_links.append((filename, href))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"  Found {len(excel_links)} Excel files\")\n",
    "    return excel_links\n",
    "\n",
    "\n",
    "def wait_for_download(temp_dir: Path, timeout: int = 60):\n",
    "    \"\"\"Wait for a download to complete.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < timeout:\n",
    "        files = list(temp_dir.iterdir())\n",
    "        \n",
    "        # Check for partial downloads (Chrome uses .crdownload)\n",
    "        partial = [f for f in files if f.suffix in ('.crdownload', '.tmp', '.part')]\n",
    "        excel = [f for f in files if f.suffix.lower() in ('.xls', '.xlsx')]\n",
    "        \n",
    "        if excel and not partial:\n",
    "            time.sleep(0.5)\n",
    "            return excel[0]\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def clear_folder(folder: Path):\n",
    "    \"\"\"Remove all files from a folder.\"\"\"\n",
    "    for f in folder.iterdir():\n",
    "        try:\n",
    "            f.unlink()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN DOWNLOAD SCRIPT\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Download all Iowa election Excel files.\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    BASE_URL = \"https://sos.iowa.gov/precinct-results-county\"\n",
    "    YEARS = [\"2016\", \"2018\", \"2020\"]\n",
    "    OUTPUT_DIR = Path(\"./iowa_election_results\")\n",
    "    TEMP_DIR = OUTPUT_DIR / \"_temp\"\n",
    "    \n",
    "    # Create folders\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    TEMP_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Files will be saved to: {OUTPUT_DIR.absolute()}\")\n",
    "    print(f\"Using browser: {BROWSER}\\n\")\n",
    "    \n",
    "    # Start the browser\n",
    "    driver = setup_driver(str(TEMP_DIR))\n",
    "    all_downloaded = []\n",
    "    \n",
    "    try:\n",
    "        # Loop through each year\n",
    "        for year in YEARS:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processing {year}...\")\n",
    "            print('='*50)\n",
    "            \n",
    "            # Build URL and find Excel links\n",
    "            page_url = f\"{BASE_URL}-{year}-general\"\n",
    "            excel_links = get_excel_links(driver, page_url)\n",
    "            \n",
    "            if not excel_links:\n",
    "                print(f\"  No files found for {year}\")\n",
    "                continue\n",
    "            \n",
    "            # Download each file\n",
    "            for i, (filename, file_url) in enumerate(excel_links, 1):\n",
    "                print(f\"  [{i}/{len(excel_links)}] Downloading: {filename}\")\n",
    "                \n",
    "                # Clear temp folder\n",
    "                clear_folder(TEMP_DIR)\n",
    "                \n",
    "                # Navigate to file URL (triggers download)\n",
    "                driver.get(file_url)\n",
    "                \n",
    "                # Wait for download\n",
    "                downloaded = wait_for_download(TEMP_DIR)\n",
    "                \n",
    "                if downloaded:\n",
    "                    # Rename with year suffix: Adair.xls -> Adair_2018.xls\n",
    "                    new_name = f\"{Path(filename).stem}_{year}{Path(filename).suffix}\"\n",
    "                    final_path = OUTPUT_DIR / new_name\n",
    "                    downloaded.rename(final_path)\n",
    "                    all_downloaded.append(final_path)\n",
    "                    print(f\"       Saved as: {new_name}\")\n",
    "                else:\n",
    "                    print(f\"       Warning: Download may have failed\")\n",
    "                \n",
    "                time.sleep(1)  # Pause between downloads\n",
    "    \n",
    "    finally:\n",
    "        print(\"\\nClosing browser...\")\n",
    "        driver.quit()\n",
    "        \n",
    "        # Clean up temp folder\n",
    "        try:\n",
    "            clear_folder(TEMP_DIR)\n",
    "            TEMP_DIR.rmdir()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"DONE! Downloaded {len(all_downloaded)} files\")\n",
    "    print(f\"Location: {OUTPUT_DIR.absolute()}\")\n",
    "    print('='*50)\n",
    "\n",
    "# Run it!\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e0872",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternative: The \"Just Loop Through a List\" Approach\n",
    "\n",
    "The code above discovers links by parsing HTML. But there's a simpler way to think about it:\n",
    "\n",
    "**You already know what you want to download.** You can see the 99 county names on the webpage. Why not just make a list and loop through it?\n",
    "\n",
    "This is closer to how you'd think about it in Stata:\n",
    "```stata\n",
    "foreach county in Adair Adams Allamakee ... {\n",
    "    * download file for `county'\n",
    "}\n",
    "```\n",
    "\n",
    "### The Simpler Approach:\n",
    "\n",
    "1. Make a list of county names\n",
    "2. For each county, build the download URL\n",
    "3. Download it\n",
    "4. Repeat for each election year\n",
    "\n",
    "**Note:** You might discover that one year has a different list of counties ‚Äî that's a real data quality issue to investigate!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41731192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SIMPLER APPROACH: Just loop through a list of counties\n",
    "# ============================================================\n",
    "\n",
    "# Iowa's 99 counties (in alphabetical order)\n",
    "IOWA_COUNTIES = [\n",
    "    \"Adair\", \"Adams\", \"Allamakee\", \"Appanoose\", \"Audubon\",\n",
    "    \"Benton\", \"Black Hawk\", \"Boone\", \"Bremer\", \"Buchanan\",\n",
    "    \"Buena Vista\", \"Butler\", \"Calhoun\", \"Carroll\", \"Cass\",\n",
    "    \"Cedar\", \"Cerro Gordo\", \"Cherokee\", \"Chickasaw\", \"Clarke\",\n",
    "    \"Clay\", \"Clayton\", \"Clinton\", \"Crawford\", \"Dallas\",\n",
    "    \"Davis\", \"Decatur\", \"Delaware\", \"Des Moines\", \"Dickinson\",\n",
    "    \"Dubuque\", \"Emmet\", \"Fayette\", \"Floyd\", \"Franklin\",\n",
    "    \"Fremont\", \"Greene\", \"Grundy\", \"Guthrie\", \"Hamilton\",\n",
    "    \"Hancock\", \"Hardin\", \"Harrison\", \"Henry\", \"Howard\",\n",
    "    \"Humboldt\", \"Ida\", \"Iowa\", \"Jackson\", \"Jasper\",\n",
    "    \"Jefferson\", \"Johnson\", \"Jones\", \"Keokuk\", \"Kossuth\",\n",
    "    \"Lee\", \"Linn\", \"Louisa\", \"Lucas\", \"Lyon\",\n",
    "    \"Madison\", \"Mahaska\", \"Marion\", \"Marshall\", \"Mills\",\n",
    "    \"Mitchell\", \"Monona\", \"Monroe\", \"Montgomery\", \"Muscatine\",\n",
    "    \"O'Brien\", \"Osceola\", \"Page\", \"Palo Alto\", \"Plymouth\",\n",
    "    \"Pocahontas\", \"Polk\", \"Pottawattamie\", \"Poweshiek\", \"Ringgold\",\n",
    "    \"Sac\", \"Scott\", \"Shelby\", \"Sioux\", \"Story\",\n",
    "    \"Tama\", \"Taylor\", \"Union\", \"Van Buren\", \"Wapello\",\n",
    "    \"Warren\", \"Washington\", \"Wayne\", \"Webster\", \"Winnebago\",\n",
    "    \"Winneshiek\", \"Woodbury\", \"Worth\", \"Wright\"\n",
    "]\n",
    "\n",
    "print(f\"Iowa has {len(IOWA_COUNTIES)} counties\")\n",
    "print(f\"First few: {IOWA_COUNTIES[:5]}\")\n",
    "print(f\"Last few: {IOWA_COUNTIES[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOWNLOAD FILES - The Simple Way\n",
    "# ============================================================\n",
    "# One loop per year. Easy to read, easy to understand.\n",
    "#\n",
    "# URL pattern:\n",
    "# https://sos.iowa.gov/elections/pdf/precinctresults/2018general/audubon.xls\n",
    "#                                                      ^^^^        ^^^^^^^\n",
    "#                                                      year        county (lowercase)\n",
    "#\n",
    "# FILE EXTENSIONS:\n",
    "#   2016 = .xlsx\n",
    "#   2018 = .xls\n",
    "#   2020 = .xlsx\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "\n",
    "# --- Setup ---\n",
    "OUTPUT_DIR = Path(\"./iowa_election_results\")\n",
    "TEMP_DIR = OUTPUT_DIR / \"_temp\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "TEMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Start the browser (uses BROWSER variable from earlier)\n",
    "browser = BROWSER.lower()\n",
    "if browser == \"chrome\":\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    options = Options()\n",
    "    options.add_experimental_option(\"prefs\", {\"download.default_directory\": str(TEMP_DIR.absolute())})\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "elif browser == \"edge\":\n",
    "    from selenium.webdriver.edge.options import Options\n",
    "    options = Options()\n",
    "    options.add_experimental_option(\"prefs\", {\"download.default_directory\": str(TEMP_DIR.absolute())})\n",
    "    driver = webdriver.Edge(options=options)\n",
    "elif browser == \"firefox\":\n",
    "    from selenium.webdriver.firefox.options import Options\n",
    "    options = Options()\n",
    "    options.set_preference(\"browser.download.dir\", str(TEMP_DIR.absolute()))\n",
    "    options.set_preference(\"browser.download.folderList\", 2)\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "print(f\"Started {browser} browser\")\n",
    "print(f\"Files will be saved to: {OUTPUT_DIR.absolute()}\\n\")\n",
    "\n",
    "# --- Helper functions (the \"plumbing\") ---\n",
    "\n",
    "def wait_for_download(timeout=60):\n",
    "    \"\"\"Wait until download completes.\"\"\"\n",
    "    start = time.time()\n",
    "    while time.time() - start < timeout:\n",
    "        files = list(TEMP_DIR.iterdir())\n",
    "        partial = [f for f in files if f.suffix in ('.crdownload', '.tmp', '.part')]\n",
    "        done = [f for f in files if f.suffix in ('.xls', '.xlsx')]\n",
    "        if done and not partial:\n",
    "            return done[0]\n",
    "        time.sleep(0.5)\n",
    "    return None\n",
    "\n",
    "def clear_temp():\n",
    "    \"\"\"Empty the temp folder.\"\"\"\n",
    "    for f in TEMP_DIR.iterdir():\n",
    "        try:\n",
    "            f.unlink()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# ============================================================\n",
    "# YEAR 2016 (.xlsx files)\n",
    "# ============================================================\n",
    "print(\"=\"*50)\n",
    "print(\"Downloading 2016 files...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for county in IOWA_COUNTIES:\n",
    "    # County name: lowercase, no spaces, no apostrophes\n",
    "    # e.g., \"O'Brien\" -> \"obrien\", \"Black Hawk\" -> \"blackhawk\"\n",
    "    county_lower = county.lower().replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    \n",
    "    # 2016 uses .xlsx format\n",
    "    url = f\"https://sos.iowa.gov/elections/pdf/precinctresults/2016general/{county_lower}.xlsx\"\n",
    "    \n",
    "    print(f\"  {county}...\", end=\" \")\n",
    "    \n",
    "    clear_temp()\n",
    "    driver.get(url)\n",
    "    downloaded = wait_for_download()\n",
    "    \n",
    "    if downloaded:\n",
    "        new_name = f\"{county}_2016.xlsx\"\n",
    "        downloaded.rename(OUTPUT_DIR / new_name)\n",
    "        print(\"‚úì\")\n",
    "    else:\n",
    "        print(\"FAILED - check if file exists on website\")\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "# ============================================================\n",
    "# YEAR 2018 (.xls files)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Downloading 2018 files...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for county in IOWA_COUNTIES:\n",
    "    # County name: lowercase, no spaces, no apostrophes\n",
    "    county_lower = county.lower().replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    \n",
    "    # 2018 uses .xls format (older Excel format)\n",
    "    url = f\"https://sos.iowa.gov/elections/pdf/precinctresults/2018general/{county_lower}.xls\"\n",
    "    \n",
    "    print(f\"  {county}...\", end=\" \")\n",
    "    \n",
    "    clear_temp()\n",
    "    driver.get(url)\n",
    "    downloaded = wait_for_download()\n",
    "    \n",
    "    if downloaded:\n",
    "        new_name = f\"{county}_2018.xls\"\n",
    "        downloaded.rename(OUTPUT_DIR / new_name)\n",
    "        print(\"‚úì\")\n",
    "    else:\n",
    "        print(\"FAILED - check if file exists on website\")\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "# ============================================================\n",
    "# YEAR 2020 (.xlsx files)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Downloading 2020 files...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for county in IOWA_COUNTIES:\n",
    "    county_lower = county.lower().replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    \n",
    "    # 2020 uses .xlsx format\n",
    "    url = f\"https://sos.iowa.gov/elections/pdf/precinctresults/2020general/{county_lower}.xlsx\"\n",
    "    \n",
    "    print(f\"  {county}...\", end=\" \")\n",
    "    \n",
    "    clear_temp()\n",
    "    driver.get(url)\n",
    "    downloaded = wait_for_download()\n",
    "    \n",
    "    if downloaded:\n",
    "        new_name = f\"{county}_2020.xlsx\"\n",
    "        downloaded.rename(OUTPUT_DIR / new_name)\n",
    "        print(\"‚úì\")\n",
    "    else:\n",
    "        print(\"FAILED - check if file exists on website\")\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "# --- Cleanup ---\n",
    "driver.quit()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DONE!\")\n",
    "print(f\"Files saved to: {OUTPUT_DIR.absolute()}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cdd52d",
   "metadata": {},
   "source": [
    "### What's Different About This Version?\n",
    "\n",
    "| Original Approach | Simpler Approach |\n",
    "|-------------------|------------------|\n",
    "| Parse HTML to find links | Already know the county names |\n",
    "| One function handles all years | Separate loop for each year (easier to read) |\n",
    "| Abstract, flexible | Concrete, obvious |\n",
    "| Adapts if website changes | Breaks if URL pattern changes |\n",
    "\n",
    "**The simpler version is easier to understand** because it matches how you think: *\"For each county in my list, download the file.\"*\n",
    "\n",
    "**The original version is more robust** because it discovers links automatically ‚Äî useful if you don't know the URL pattern in advance.\n",
    "\n",
    "### Did Any Downloads Fail?\n",
    "\n",
    "If a county shows \"FAILED\", investigate:\n",
    "- Does that county exist for that year?\n",
    "- Is the URL pattern different?\n",
    "- Is it a PDF instead of Excel?\n",
    "\n",
    "**This is real data work** ‚Äî discovering inconsistencies and handling them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681f8ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Learned\n",
    "\n",
    "### The Process\n",
    "\n",
    "1. We tried to download files using Python's `requests` library\n",
    "2. The website blocked us (403 Forbidden) because it detected we were a script\n",
    "3. We used **Selenium** to control a real browser, which the website accepts\n",
    "\n",
    "### Key Concepts (Transferable to R/Stata)\n",
    "\n",
    "| Concept | What it means |\n",
    "|---------|---------------|\n",
    "| **Loop** | Repeat an action for each item in a list |\n",
    "| **String building** | Create text by combining pieces (URLs, filenames) |\n",
    "| **Conditional logic** | Only do something if a condition is true |\n",
    "| **HTTP status codes** | 200=OK, 403=Forbidden, 404=Not Found |\n",
    "| **HTML parsing** | Reading the structure of a webpage to find links |\n",
    "\n",
    "### Python-Specific Tools\n",
    "\n",
    "| Tool | Purpose | R Equivalent |\n",
    "|------|---------|--------------|\n",
    "| `requests` | Download web pages | `httr::GET()` |\n",
    "| `BeautifulSoup` | Parse HTML | `rvest::read_html()` |\n",
    "| `selenium` | Control a browser | `RSelenium` |\n",
    "| `pathlib.Path` | Work with file paths | `fs` package |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that we have the Excel files downloaded, **Stage 2** will:\n",
    "- Read each Excel file\n",
    "- Extract the data we need\n",
    "- Combine everything into one clean dataset\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Challenge: Did We Get Everything?\n",
    "\n",
    "After running the download, check your results:\n",
    "\n",
    "1. **How many files did you download?** (Should be ~99 per year √ó 3 years)\n",
    "2. **Are all 99 Iowa counties represented for each year?**\n",
    "\n",
    "If something is missing, investigate:\n",
    "- Go to the original webpage in your browser\n",
    "- Look at what files are actually listed\n",
    "- Is everything an Excel file? Or is there something different?\n",
    "\n",
    "**Real-world data is messy.** Sometimes files are in unexpected formats, links are broken, or naming conventions change. Part of data analysis is discovering these issues and deciding how to handle them.\n",
    "\n",
    "---\n",
    "*Questions? Ask your instructor!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
